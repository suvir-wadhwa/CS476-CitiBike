import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder().appName("CitiBike Analysis").getOrCreate()

val data2019 = spark.read.option("header", "true").csv("hdfs:///user/sw4854_nyu_edu/201809-citibike-tripdata.csv")
val data2023 = spark.read.option("header", "true").csv("hdfs:///user/sw4854_nyu_edu/JC-202309-citibike-tripdata.csv")

val cleanedDf2019 = data2019.selectExpr(
  "cast(`tripduration` as string) as `ride_id`",
  "cast(`usertype` as string) as `rideable_type`",
  "cast(`starttime` as timestamp) as `started_at`",
  "cast(`stoptime` as timestamp) as `ended_at`",
  "cast(`start station name` as string) as `start_station_name`",
  "cast(`start station id` as string) as `start_station_id`",
  "cast(`end station name` as string) as `end_station_name`",
  "cast(`end station id` as string) as `end_station_id`",
  "cast(`start station latitude` as double) as `start_lat`",
  "cast(`start station longitude` as double) as `start_lng`",
  "cast(`end station latitude` as double) as `end_lat`",
  "cast(`end station longitude` as double) as `end_lng`",
  "cast(`bikeid` as string) as `member_casual`"
)

val cleanedDf2023 = data2023.selectExpr(
  "cast(`tripduration` as string) as `ride_id`",
  // ... repeat for all other fields, ensuring column names match the 2023 dataset ...
)

cleanedDf2019.printSchema()

val count2019 = cleanedDf2019.count()
val count2023 = cleanedDf2023.count()

val popularStations2019 = cleanedDf2019.groupBy("start_station_name").count().orderBy(desc("count"))
val popularStations2023 = cleanedDf2023.groupBy("start_station_name").count().orderBy(desc("count"))

popularStations2019.show()
popularStations2023.show()

val memberType2019 = cleanedDf2019.groupBy("member_casual").count()
val memberType2023 = cleanedDf2023.groupBy("member_casual").count()

memberType2019.show()
memberType2023.show()

val cleanedDf2019WithDuration = cleanedDf2019.withColumn("trip_duration", unix_timestamp($"ended_at") - unix_timestamp($"started_at"))
val cleanedDf2023WithDuration = cleanedDf2023.withColumn("trip_duration", unix_timestamp($"ended_at") - unix_timestamp($"started_at"))

cleanedDf2019WithDuration.groupBy("trip_duration").count().orderBy(desc("count")).show()
cleanedDf2019WithDuration.select(avg($"trip_duration")).show()

cleanedDf2023WithDuration.groupBy("trip_duration").count().orderBy(desc("count")).show()
cleanedDf2023WithDuration.select(avg($"trip_duration")).show()

val cleanedDf2019WithHour = cleanedDf2019WithDuration.withColumn("start_hour", hour($"started_at"))
val cleanedDf2023WithHour = cleanedDf2023WithDuration.withColumn("start_hour", hour($"started_at"))

cleanedDf2019WithHour.groupBy("start_hour").count().orderBy("start_hour").show()
cleanedDf2023WithHour.groupBy("start_hour").count().orderBy("start_hour").show()
